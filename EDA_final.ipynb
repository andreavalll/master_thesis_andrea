{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {
    "id": "0"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import ast\n",
    "import re\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {
    "id": "1"
   },
   "source": [
    "### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {
    "id": "2"
   },
   "source": [
    "EDA steps:\n",
    "- Stereotype distribution -> ensure merged dataset is balanced\n",
    "- Sentence length distribution and removal of outliers -> helps calibrate prompt size and model input limits\n",
    "- Lexical content per group -> early signal of stereotypical phrasing\n",
    "    - preprocessing + tokenization + word frequency analysis\n",
    "- Initial t-SNE plot -> preview embedding structure before full analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3",
    "outputId": "5a30f46c-c75e-40d7-a1e0-bd6729e6f7ce"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {
    "id": "4"
   },
   "outputs": [],
   "source": [
    "# Data Preparation and Creation of merged dataset in preprocessing.ipynb file\n",
    "file_path = '/content/drive/MyDrive/LLM_thesis/merged_dataset.parquet'\n",
    "merged_df = pd.read_parquet(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {
    "id": "5"
   },
   "source": [
    "#### Stereotype Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 617
    },
    "id": "6",
    "outputId": "5231f935-1c7f-4587-eac7-718c113d29e8"
   },
   "outputs": [],
   "source": [
    "counts = merged_df['stereotype'].value_counts()\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 807
    },
    "id": "7",
    "outputId": "a2cfa1bf-2363-46fa-b891-3f4fd841290a"
   },
   "outputs": [],
   "source": [
    "counts = merged_df['stereotype'].value_counts().sort_index()\n",
    "percentages = merged_df['stereotype'].value_counts(normalize=True).sort_index() * 100\n",
    "\n",
    "df_counts = pd.DataFrame({\n",
    "    \"count\": counts,\n",
    "    \"percentage\": percentages\n",
    "})\n",
    "# Mapping stereotype labels and gender groups\n",
    "label_map = {\n",
    "    1: \"Women are emotional and irrational\",\n",
    "    2: \"Women are gentle, kind, and submissive\",\n",
    "    3: \"Women are empathetic and caring\",\n",
    "    4: \"Women are neat and diligent\",\n",
    "    5: \"Women are social\",\n",
    "    6: \"Women are weak\",\n",
    "    7: \"Women are beautiful\",\n",
    "    8: \"Men are tough and rough\",\n",
    "    9: \"Men are self-confident\",\n",
    "    10: \"Men are professional\",\n",
    "    11: \"Men are rational\",\n",
    "    12: \"Men are providers\",\n",
    "    13: \"Men are leaders\",\n",
    "    14: \"Men are childish\",\n",
    "    15: \"Men are sexual\",\n",
    "    16: \"Men are strong\"\n",
    "}\n",
    "df_counts[\"label\"] = df_counts.index.map(label_map)\n",
    "df_counts[\"group\"] = df_counts.index.map(lambda x: \"Women\" if x <= 7 else \"Men\")\n",
    "\n",
    "# Setting colors based on group\n",
    "color_map = {\"Women\": \"lightpink\", \"Men\": \"lightblue\"}\n",
    "colors = df_counts[\"group\"].map(color_map)\n",
    "# Sorting\n",
    "df_counts = df_counts.sort_values(\"count\", ascending=True)\n",
    "colors = colors.loc[df_counts.index]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "bars = plt.barh(df_counts[\"label\"], df_counts[\"count\"], color=colors)\n",
    "plt.xlabel(\"Frequency\")\n",
    "plt.title(\"Stereotype Category Distribution by Gender\")\n",
    "\n",
    "for bar, pct in zip(bars, df_counts[\"percentage\"]):\n",
    "    plt.text(bar.get_width() + 2, bar.get_y() + bar.get_height()/2, f\"{pct:.1f}%\", va='center')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"stereotype_distribution.png\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {
    "id": "8"
   },
   "source": [
    "The distribution of gender stereotypes in the dataset is relatively balanced, with percentages ranging from 5.5 to 7.7, indicating no major outliers and a good representation of all 16 stereotype categories. The most represented stereotype is “Women are empathetic and caring” (7.7%), followed by “Men are tough and rough” (7.1%) and “Women are emotional and irrational” (7.0%). The least represented stereotypes are “Men are childish”, “Women are weak”, and “Women are social”, all around 5.5%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {
    "id": "9"
   },
   "source": [
    "#### Sentence length distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "10",
    "outputId": "07615bce-75a5-4f4d-8913-db36f7d9610d"
   },
   "outputs": [],
   "source": [
    "# Computes sentence lengths (word count)\n",
    "merged_df[\"length\"] = merged_df[\"sentence\"].apply(lambda x: len(str(x).split()))\n",
    "# Plot histogram for overall sentence lengths\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(merged_df[\"length\"], bins=30, color=\"seagreen\", edgecolor=\"black\")\n",
    "plt.xlabel(\"Sentence Length (words)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Overall Sentence Length Distribution\")\n",
    "plt.savefig(\"sentence_length_distribution.png\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {
    "id": "11"
   },
   "source": [
    "Most sentences have around 10 words, with the majority falling under the 6-13 range. A few outliers are evident at both extremes (2 or 30 words).\n",
    "This indicates a fairly well-distributed sentence length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "12",
    "outputId": "52a7a95c-42b8-49da-b417-4145d4794d2f"
   },
   "outputs": [],
   "source": [
    "# Based on my embedding and prompt-based methodology, removing outliers makes sense. Outliers may distort cosine similarity scores, especially during average pooling.\n",
    "# Computing Q1, Q3, and IQR\n",
    "q1 = merged_df[\"length\"].quantile(0.25)\n",
    "q3 = merged_df[\"length\"].quantile(0.75)\n",
    "iqr = q3 - q1\n",
    "\n",
    "# Defining outlier bounds\n",
    "lower_bound = q1 - 1.5 * iqr\n",
    "upper_bound = q3 + 1.5 * iqr\n",
    "\n",
    "# Filtering the dataset\n",
    "filtered_df = merged_df[(merged_df[\"length\"] >= lower_bound) & (merged_df[\"length\"] <= upper_bound)]\n",
    "\n",
    "print(f\"Original size: {merged_df.shape[0]}\")\n",
    "print(f\"Filtered size: {filtered_df.shape[0]}\")\n",
    "print(f\"Removed outliers: {merged_df.shape[0] - filtered_df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {
    "id": "13"
   },
   "outputs": [],
   "source": [
    "filtered_df.to_parquet(\"filtered_df.parquet\", index=False) #save to parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {
    "id": "14"
   },
   "source": [
    "#### Sentence preprocessing, tokenization & word frequency analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pdPnh_zog827",
    "outputId": "48562717-a7c3-4ab9-a2ff-ed2ca1d6cc57"
   },
   "outputs": [],
   "source": [
    "nltk.download('punkt_tab')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "15",
    "outputId": "6b2fa22d-832a-4713-e8da-17b4d11f245c"
   },
   "outputs": [],
   "source": [
    "# Defining stopwords with some additions\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "additional_stops = {\"like\", \"could\", \"would\", \"im\", \"get\", \"got\", \"one\", \"also\", \"dont\", \"didnt\", \"even\", \"always\", \"never\"}\n",
    "stop_words.update(additional_stops)\n",
    "\n",
    "# Preprocessing and tokenizing all sentences\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z\\s]\", \"\", text)  # removes punctuation\n",
    "    tokens = word_tokenize(text)\n",
    "    return [t for t in tokens if t not in stop_words]\n",
    "\n",
    "tokens = []\n",
    "for sentence in filtered_df[\"sentence\"]:\n",
    "    tokens.extend(preprocess(sentence))\n",
    "\n",
    "# Counting word frequencies\n",
    "word_counts = Counter(tokens)\n",
    "freq_df = pd.DataFrame(word_counts.items(), columns=[\"word\", \"count\"]).sort_values(by=\"count\", ascending=False)\n",
    "\n",
    "# Show top and bottom 15\n",
    "print(\"Most frequent words:\\n\", freq_df.head(15))\n",
    "print(\"\\nLeast frequent words:\\n\", freq_df.tail(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {
    "id": "uMOjlqPASbfa"
   },
   "outputs": [],
   "source": [
    "freq_df.to_csv(\"word_frequencies.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "16",
    "outputId": "f0268311-4020-4dc2-fb65-0db469831933"
   },
   "outputs": [],
   "source": [
    "# Now let's see the top 5 words per stereotype category:\n",
    "stereotype_top_words = {}\n",
    "\n",
    "# Looping through each stereotype category\n",
    "for cat in sorted(filtered_df[\"stereotype\"].unique()):\n",
    "    cat_sentences = filtered_df[filtered_df[\"stereotype\"] == cat][\"sentence\"]\n",
    "\n",
    "    tokens = []\n",
    "    for sentence in cat_sentences:\n",
    "        tokens.extend(preprocess(sentence))\n",
    "\n",
    "    word_counts = Counter(tokens)\n",
    "    top_words = word_counts.most_common(5)\n",
    "\n",
    "    stereotype_top_words[cat] = top_words\n",
    "\n",
    "for cat, words in stereotype_top_words.items():\n",
    "    print(f\"Stereotype {cat}: {words}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {
    "id": "17"
   },
   "source": [
    "The top 5 most frequent words in each stereotype category strongly align with the associated stereotype label. For example, Stereotype 3 (“Women are empathetic and caring”) includes care, help, and children, while Stereotype 10 (“Men are professional”) includes work, job and professional. These lexical patterns validate the mappings I made from StereoSet to GEST, confirming that the merged dataset preserves semantic coherence across categories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {
    "id": "18"
   },
   "source": [
    "#### Initial t-SNE plot -> Preview embedding structure before full analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {
    "id": "19"
   },
   "source": [
    "Sources:\n",
    "- https://huggingface.co/docs/transformers/en/model_doc/gpt2\n",
    "- https://stackoverflow.com/questions/77748737/how-to-calculate-word-and-sentence-embedding-using-gpt-2\n",
    "- https://stackoverflow.com/questions/52849890/how-to-implement-t-sne-in-a-model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {
    "id": "20"
   },
   "source": [
    "#### Starting with gpt-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {
    "id": "21"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "from sklearn.manifold import TSNE\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265,
     "referenced_widgets": [
      "74a1b6890a4f486b9f3d100aed5c2065",
      "9375c395184041c9897c448f4d5818cc",
      "f808020597aa4a5a9d93ae7e6bd9b5ce",
      "e08651a7bd8947749d371830c7fbbf4b",
      "c3140c7aac964074b634792a15011c75",
      "bbdc29d694284105ad41b5ba78769e8f",
      "7befd4be7bf04e7cba373d3ddfe0ce3a",
      "de5fc2b6c1184d34a5c3b37dcf9ee98f",
      "4e7766294973448593cf927ff87a3636",
      "adbaf86cd1464195be2a13dad98a2bed",
      "8e1081e5ae404abd8b3fa6db30e771c3",
      "df70b677c7524eb8a7f814b6235cbf3f",
      "9996d67c3a154047a8ad9878090cd0d5",
      "6ca986710ec34a539993b43333ff773b",
      "e09f9dc29f4b4a1899200ce0fc0bdc49",
      "6ee07917e4bc4e088cc4bfbe7cfe8467",
      "a9aac2024348446384a698338d65b00a",
      "43d441fa3a1045fd987a0e7574d4894b",
      "e4be0e8149b741748bc91a6f3d9766ad",
      "a1fb501615f74a02ba812b5c7bf112ff",
      "ccf79c5e3b8544c7bd35864b6e8cf467",
      "35958630f1a24b26b4b9cf44bcc36ccc",
      "4de67283b116441a979358afab4b384c",
      "7ae0898fd5494f94abde9afd30e6f6d6",
      "268e7ed5d31a40cd9719b8695d23d0e6",
      "bb00964303544f19bde1048784316235",
      "ee2cc23cbe424a77b724e5a3c20f30db",
      "9f490db53d2d4ca5a777b3da3b8a03f5",
      "aca7a19a3aa64c1488cd0d48a865c57e",
      "1037821a324c4b60af2a5f718478f078",
      "bb9176b17d1d4644bb9db90b94419bfd",
      "c5d4782797cc44bf9ac35fcc823146d5",
      "f6dd89b9ea7c44d5aea41e3a5b7d800a",
      "62aeb3c68aec47fb920b832d15d7de67",
      "6c225f7ace8149ba8e3708e8f1e09cde",
      "c80b0ff6ea3c4b26a60bb21946ba5fbc",
      "5c9d6b8e297f47668055f3d1a8afa008",
      "122efc3f9fec4dfd86e5f13bdb1e37d5",
      "2af9b7f86bc94c23972f0adf015da120",
      "914c4158cb7a4f328bc0d0dcb7c360ed",
      "1e05f2e9b0d64171a08e3410a38d845e",
      "1d25d906b3f64af2981eb205ab32c67a",
      "4274883cf00549e78641e455a0415d53",
      "3fed5a9758404d05a3ca2f259d33ea08",
      "4069804d7acb4682a2cff99cce5b6255",
      "c0840acaa45a45aeb0579bd89bd0d162",
      "1ad1f7aace8048bd9e06c296e0139c84",
      "1702dd1bca1a47c4a72959c9a169bad7",
      "86c413abfd9243d1a32df8fb5bbca113",
      "0b5e29f7068d4338a5853003127e01c8",
      "70fddb1753024907839e9f75fd9ca4d9",
      "d29fc21a973f41349b2615373fe10f1e",
      "8f1a975da08841588c9a601dc89cfcb2",
      "1ff8fa152b6b41a4bf2b26ec4b995802",
      "a4e257d2ee2946878632c48035d0aeae",
      "d28626381c774c759277b048a56f82e1",
      "81bf7bf26ba54b5b8ae26edfcc244fba",
      "92a0e2e91eb545b7a8294c0890e49723",
      "e8e1c125a5ec42279ebd4cd7e2680512",
      "45c1a3a29ef24c5ca6fb4a2f15be0cf0",
      "6e920faf814a4b40b90dda0a53ece2dc",
      "66c79638e852427fa0c1c64001b1d115",
      "c6cec57583a24f558c5aa9436a70f349",
      "c872f2d0754a4ef495d25be1cbe24746",
      "2cc9308d2c1b45339f24f2735c9e0722",
      "fbbd900c64b94d38ac7f2be36a4d1610"
     ]
    },
    "id": "22",
    "outputId": "96cf501f-487a-4970-ad59-a872a009d0a9"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
    "model = GPT2Model.from_pretrained(\"openai-community/gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "23",
    "outputId": "00681e5d-ea42-47ea-af5e-5b84340d346a"
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {
    "id": "24"
   },
   "outputs": [],
   "source": [
    "# Getting average-pooled sentence embedding from GPT-2\n",
    "def get_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=128).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        last_hidden_state = outputs.last_hidden_state\n",
    "        embedding = last_hidden_state.mean(dim=1).squeeze()\n",
    "    return embedding.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "25",
    "outputId": "dc39b207-6c30-4d24-c25e-6f13c30de095"
   },
   "outputs": [],
   "source": [
    "# Extracting embeddings for all sentences\n",
    "embeddings = []\n",
    "for sentence in tqdm(filtered_df[\"sentence\"], desc=\"Extracting GPT-2 embeddings\"):\n",
    "    emb = get_embedding(sentence)\n",
    "    embeddings.append(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "26",
    "outputId": "e0953e4b-da15-4110-ea0b-8651a6922f83"
   },
   "outputs": [],
   "source": [
    "embeddings_df = pd.DataFrame(embeddings)  # shape = (n_sentences, 768)\n",
    "embeddings_df[\"stereotype\"] = filtered_df[\"stereotype\"].values # added 1 col\n",
    "embeddings_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {
    "id": "27"
   },
   "outputs": [],
   "source": [
    "# Applying t-SNE\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "tsne_result = tsne.fit_transform(embeddings_df.drop(columns=[\"stereotype\"]).values)\n",
    "embeddings_df[\"tsne_x\"] = tsne_result[:, 0]\n",
    "embeddings_df[\"tsne_y\"] = tsne_result[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "28",
    "outputId": "da12f45d-da85-4ff6-e7aa-7d1e60aece18"
   },
   "outputs": [],
   "source": [
    "# Plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "for cat in sorted(embeddings_df[\"stereotype\"].unique()):\n",
    "    subset = embeddings_df[embeddings_df[\"stereotype\"] == cat]\n",
    "    plt.scatter(subset[\"tsne_x\"], subset[\"tsne_y\"], label=f\"Cat {cat}\", alpha=0.7)\n",
    "plt.title(\"t-SNE of GPT-2 Embeddings\")\n",
    "plt.xlabel(\"t-SNE 1\")\n",
    "plt.ylabel(\"t-SNE 2\")\n",
    "plt.legend()\n",
    "plt.savefig(\"GPT-2_Embeddings.png\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {
    "id": "29"
   },
   "source": [
    "I don't see any clusters forming, I think 16 categories might be too many for a 2D representation so that's why they are not separating clearly. Lets try clustering my gender so from categories 1-7 (female stereotypes) and 8-16 (male stereotypes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "30",
    "outputId": "e88c0270-93f2-4708-f718-16172bf54e21"
   },
   "outputs": [],
   "source": [
    "embeddings_df[\"gender_group\"] = embeddings_df[\"stereotype\"].apply(lambda x: \"Women\" if x <= 7 else \"Men\")\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "for group in sorted(embeddings_df[\"gender_group\"].unique()):\n",
    "    subset = embeddings_df[embeddings_df[\"gender_group\"] == group]\n",
    "    plt.scatter(subset[\"tsne_x\"], subset[\"tsne_y\"], label=group, alpha=0.7)\n",
    "plt.title(\"t-SNE of GPT-2 Embeddings by Gender Group\")\n",
    "plt.xlabel(\"t-SNE 1\")\n",
    "plt.ylabel(\"t-SNE 2\")\n",
    "plt.legend()\n",
    "plt.savefig(\"GPT-2_Embeddings_gender.png\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {
    "id": "31"
   },
   "source": [
    "The t-SNE plot shows overlapping male and female stereotype embeddings in GPT-2, suggesting no clear separation at the sentence level. This motivates deeper analysis (e.g., cosine similarity, prompt completions) to uncover more subtle forms of gender bias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {
    "id": "32"
   },
   "source": [
    "### Now LLaMA 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17,
     "referenced_widgets": [
      "6550d6b00b1f438e9549c7789bf4c44e",
      "512daa4d9c98402fa265fb01b889f453",
      "0d8a38c6c9c24aec901d5d8b2e68ad35",
      "34b62f9ea0234c7fbf5dba18bab0693e",
      "de37b53a50154a9485a98beab1074427",
      "dde4fa4b5a324e1e9284325851bef78b",
      "ed90d55c06de4ef29883f83861f3d0d3",
      "f379ff2ce3874694870517ab8c59cf36",
      "d345672c855a4db29fdec0118fc0067c",
      "e90ef9c97f114867b136c151f1ba7fd0",
      "4f7aad0024bf493ebe0d61be19444ab5",
      "805814c29f7e4d0482a1ef02cbbf2b89",
      "3d1a5ebe55504c8dbbeadf3641131b95",
      "10d7463d6d264f1fa02f4e369ece041b",
      "283363e1b69745769d603c25b14234c3",
      "ce051130b7e64dbaa657c7cfdc747692",
      "62971b46eb204d27b9f411513c3493ab",
      "0315549118cb4edc8f647567ca8277f2",
      "a1fb4522d31d436f86bf43d9650bd047",
      "2815f9da226447c1968404201b08e239"
     ]
    },
    "id": "33",
    "outputId": "f40f1e22-a7b8-4a53-9a6e-eb062852503f"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 970,
     "referenced_widgets": [
      "7cc07d9d168f4656a175b673fa63fa46",
      "b172190eec2b4238991aac41b1683de2",
      "24580c5566744c208dd1c226428a214f",
      "07b12e6a4cb5405fb5c104d4605fe63c",
      "51b28c8895f44e3493b5ff4d920c9e62",
      "68d11e51c27f49279a86035c6b773003",
      "d8b6d22e8b9e4cffab028cc39102a0cc",
      "7db79777b585451990c46a4d0d135262",
      "d43aa0087ad14ca78630ec9b80d5551d",
      "5db64301e1b441eaaedc8fd7b5da2dbf",
      "f0b8060ca2e24236a40a838eaedc1e28",
      "f547978bcb6847afb1099bb76ee8d224",
      "76efc229e51c43ed95cc4f914e9ad77f",
      "96873d47f26344928cf4f348cd7512cb",
      "ce7913ca872c4151b48a9c4f9ee464e9",
      "3d5e1c404dda48d6b06d4fffe5cfca1d",
      "137d6f77a02548c1be41d50c01ebd0d1",
      "0f3167dc7ee34a5887d026c010cebbcd",
      "174331c098474ea48827561f7cce6385",
      "8e07046cd2ec4097814160ceca530ef9",
      "4dec9ce61bf64b8188a3200ec94c58b3",
      "e8c7f9cc3bd74b4a9b7535af3a1ea1b5",
      "ea27e49464a34e86b6037e253d9aac34",
      "bc9290195e5346da897db86454c1131d",
      "b55c7769022844159cea41b8669aa204",
      "ac3035c63eec433d9ca1f1f480cc1cfb",
      "a46ed16892db4a608009aac4dcfcda20",
      "8023483583fe4ffabe91cfc46bce0ee7",
      "a5c153274f0849ff91b0e3bac243d497",
      "2ebc0e0574694e1a9e882605a228f814",
      "d77ad12dc7bb4a7f8cfb6dfd8b27a4b3",
      "15dd85f2480e45d4b42fd81f9afa3bd8",
      "bec83cfe1e614912a1d9a09c245d9f32",
      "35565d5a46674f91afc48bacf36aaa99",
      "ac5636663f7b4812a410c2892fea99ab",
      "e0bb4882bc1d40fba16a989de584d47b",
      "0a50de3d7e0d44589ab911cbb3746b45",
      "20fad22b3b9b4ac8a79a6c514e0e11fb",
      "870cda63a10b492a9f9518e954ea106a",
      "f51cff1b033d455b8c9c2fee80be8e0f",
      "f9af9a3547b843c5979954f0d7a89439",
      "6e3c23c3ca4247bc835069219cf4fc07",
      "05eb2531dc704284a2cb60cd39273f43",
      "b2bad3f51c794eda9389fe63ecf8ba9c",
      "86ed1a9d0457452bad4c67412b470959",
      "163ddf53120d4c4b91999eb09288018d",
      "342bc5ffa42d45df8126cf133f1faa68",
      "9c575c05d7f14695a5b1e689cc274caa",
      "f68e609b247b443b8e99b3536ba96575",
      "5087f514f02e4c51b1ce19e72785c000",
      "22e4f01b78324267bdf384f645ea5066",
      "4e67fe5c218f4694a0f050501ab90e8f",
      "e80dafcfd63c47888849c5cfcefc952c",
      "0ff8f0eec41c4e758533d76e6399d0e9",
      "a7ab59aa95b84ff4ac6529eedc185c5e",
      "c67af10f816d455782c18c967de8aed1",
      "34b334e428234f3bb006622f1f840a62",
      "cf95c97e98a84f1686c1ce086bba88cc",
      "9790b58183a547a8809d7b1f72efb6cc",
      "ebac11cf197543b9a3f07814093171bd",
      "8f6bbd20d08c4194a9e4ba732b0627ee",
      "c6f98caa38df4165a26a6120ec472563",
      "d9cdb13755f349788e2e7eae45507fa3",
      "5843a26b4d784121992eb1914a4bff41",
      "76b11b5247b1421cb49743d95c093c8c",
      "4b57497af14b432a8acd248c3f218d2c",
      "fec6e3a0a16f4a2291169f2cbf2afaad",
      "6d9c7998c09c4848b57a92ffab428fb7",
      "9d9171cb0a9b4524b0fe1a5f85e1ddc0",
      "8a10588a9d7242478e354bde4f79576a",
      "a3fbf6f799bd49b190a36a955f197d5e",
      "6bbc34d91f1f4a7d86bfa9ba2c380edf",
      "bbbecdf096024f298012175a5199daae",
      "066cd5474e384f8e94cf739411582804",
      "c2ca5664765c4e958f1857a8071f05c1",
      "860551f86ab845d5bc716e1fdb32e2c3",
      "c6bbbef1021747529701d032d8d2b185",
      "48e5785f5c8042468dcb586e8e2633e5",
      "611823cafb8245e6a9bf459ebc253b5e",
      "401da780481b4d6fb67921a03dce5a7d",
      "31db23b3ade34370af80c32455c28655",
      "110703944711464fb6c7c8ddaf1b78b2",
      "b85693437d8d44f988f50b9d0bd5166b",
      "13adee054ece4a66b18038446787ae94",
      "95abcdf58d7f46a7bcd89ac49809b5f1",
      "e199f113c01849deaa75795b159defac",
      "e32c998dafa946caa595bc830f6a7bf5",
      "c00737fb2e2741b99a6abdb1f6125df6",
      "9343b4a3b9bd42dbb091394331049571",
      "7150f3ef59754ed3bf21dff04580a8c6",
      "9acd371a31174dd18b282b7ac3ec9c5e",
      "0b5f1818cf7a4387a371ad1074245309",
      "6da99aa9bff14da0bbde894eaed0d60e",
      "179a278955dc47fc95cbcb6d8203d76a",
      "b712870f604f42daa3523ea51bf2b349",
      "2ea98d15292248fd8fd59063a3ef480b",
      "b46d5ffdcf7449e9b13e296e2e9f0aa3",
      "69e13359889b40dbbf1504970e50c50a",
      "b7a617232f2e49e083d7452c9ffd244b",
      "9ad7e8c3dfd246fbb035b33a077c3751",
      "62afddfdb3a54c409dac04ce1d411f0d",
      "15219da69b5c479a8510c8064c0d7cf0",
      "bebedd1be35c4721a645dc3436fcd8a2",
      "16cb3a5315564418abf14c9e3d352fc9",
      "4cc62b7c0acb4cfbb7442dfc90a92d44",
      "508a9ec2cb104171891d7d3f813c09de",
      "be091dd9ef5b4d45ae37bc0d25c109a7",
      "e9a219d817e44e02823f2eed5818039d",
      "70277ea082574e069030e5cb0f3e26b7",
      "bee18bf0a1df4d52ac973edba2c5cf55",
      "34b91ee7d9bb434186cffadec125ac78",
      "adbf1f408b2d4476bdfcfac6a8bce1e5",
      "d2061df614ac44978afb0d9813c5ddcc",
      "89d394e45235468a9210c8265e021468",
      "c9fe4aab84244198ad45f81e2aaf4bbb",
      "6335899d2de9468ca36eef9768faa547",
      "87c60aadb5444a0092e4a8b2a9c4fe07",
      "28e54e6ef9cc4765ae245222a7d98ebe",
      "95074a6e88bc402d90b3cc531cc5132d",
      "dc1c0a27a3e742df912c9aedd4c775d5",
      "c82868d6d6964a1ba9f063b77e23ff15"
     ]
    },
    "id": "34",
    "outputId": "70e3c123-088d-4e39-a126-508a5c750c02"
   },
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "llama_tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "llama_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"            # automatically sends model to GPU if available\n",
    ")\n",
    "llama_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "35",
    "outputId": "2575abbd-119a-409f-bccc-ba37591da211"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "llama_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "36",
    "outputId": "c0362b78-c180-4ee3-faa2-eb2883f3baa3"
   },
   "outputs": [],
   "source": [
    "# # Getting average-pooled sentence embedding from LlaMA-2 -> similar struct to gpt-2 but the hidden layers are called differently\n",
    "def get_llama2_embedding(text, model, tokenizer, device):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=128).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_hidden_states=True)\n",
    "        hidden_states = outputs.hidden_states[-1]  # last layer\n",
    "        embedding = hidden_states.mean(dim=1).squeeze()\n",
    "    return embedding.cpu().numpy()\n",
    "\n",
    "# Extracting embeddings for all sentences\n",
    "llama_embeddings = []\n",
    "for sentence in tqdm(filtered_df[\"sentence\"], desc=\"Extracting LLaMA embeddings\"):\n",
    "    emb = get_llama2_embedding(sentence, llama_model, llama_tokenizer, device)\n",
    "    llama_embeddings.append(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "37",
    "outputId": "32d5d5d6-a54a-4958-eba5-8b4584e7eb20"
   },
   "outputs": [],
   "source": [
    "embeddings_llama = pd.DataFrame(llama_embeddings)  # shape\n",
    "embeddings_llama[\"stereotype\"] = filtered_df[\"stereotype\"].values # added 1 col\n",
    "embeddings_llama.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {
    "id": "38"
   },
   "outputs": [],
   "source": [
    "# Applying t-SNE\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "tsne_result = tsne.fit_transform(embeddings_llama.drop(columns=[\"stereotype\"]).values)\n",
    "embeddings_llama[\"tsne_x\"] = tsne_result[:, 0]\n",
    "embeddings_llama[\"tsne_y\"] = tsne_result[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 863
    },
    "id": "39",
    "outputId": "26e4202e-27c6-4076-fb78-7b5cb203e9db"
   },
   "outputs": [],
   "source": [
    "# Get unique categories\n",
    "categories = sorted(embeddings_llama[\"stereotype\"].unique())\n",
    "num_categories = len(categories)\n",
    "\n",
    "# Use a colormap with enough distinguishable colors\n",
    "cmap = cm.get_cmap('tab20', num_categories)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "for i, cat in enumerate(categories):\n",
    "    subset = embeddings_llama[embeddings_llama[\"stereotype\"] == cat]\n",
    "    plt.scatter(\n",
    "        subset[\"tsne_x\"],\n",
    "        subset[\"tsne_y\"],\n",
    "        label=f\"Cat {cat}\",\n",
    "        color=cmap(i),\n",
    "        alpha=0.7,\n",
    "        s=30\n",
    "    )\n",
    "\n",
    "plt.title(\"t-SNE of LLaMA Embeddings\")\n",
    "plt.xlabel(\"t-SNE 1\")\n",
    "plt.ylabel(\"t-SNE 2\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"LlaMA_Embeddings.png\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {
    "id": "k-JtQdIGom0A"
   },
   "source": [
    "Since GPT-2 had embeddings that were very uniformly spread I didn't think it was necessary to specifically look into the different points. But, for LLaMA-2, I ensured that each category was assigned a distinct color to better differentiate the points in the t-SNE plot.\n",
    "Around tsne 0 there is a cluster of Cat 4 (Women are neat and diligent).\n",
    "I can also observe more clusters of Cat 6 throughout the plot. There is one specifically at around tsne1 40 to 60, tsn2 0.\n",
    "These clusters may indicate that the model has more structured internal representations of certain stereotypes and may be more susceptible towards biased associations. This highlights the need to delve deeper!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "41",
    "outputId": "55f34ddb-a8e0-4f54-e702-8fef24a96792"
   },
   "outputs": [],
   "source": [
    "embeddings_llama[\"gender_group\"] = embeddings_llama[\"stereotype\"].apply(lambda x: \"Women\" if x <= 7 else \"Men\")\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "for group in sorted(embeddings_llama[\"gender_group\"].unique()):\n",
    "    subset = embeddings_llama[embeddings_llama[\"gender_group\"] == group]\n",
    "    plt.scatter(subset[\"tsne_x\"], subset[\"tsne_y\"], label=group, alpha=0.7)\n",
    "plt.title(\"t-SNE of LLaMA-2 Embeddings by Gender Group\")\n",
    "plt.xlabel(\"t-SNE 1\")\n",
    "plt.ylabel(\"t-SNE 2\")\n",
    "plt.legend()\n",
    "plt.savefig(\"LlaMA_Embeddings_group.png\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {
    "id": "42"
   },
   "source": [
    "In this plot, the orange dots (Women) seem to concentrate more around 0 to -40 for tsne2 an between -20 to 20 for tsn1, while some blue areas (Men) show up on the far right (t-SNE 1 from 40 to 60). It’s not a perfect separation, but it's clearer than what I observed with GPT-2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {
    "id": "43"
   },
   "source": [
    "This suggests that LLaMA-2 might already be encoding some gender-related differences in its embeddings. So even before doing anything more complex like cosine similarity or prompt completions, the embeddings already hint at underlying biases or at least a better internal differentiation between stereotype-related embeddings— which makes a stronger case for looking deeper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {
    "id": "44"
   },
   "source": [
    "### Conclusions from EDA:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {
    "id": "45"
   },
   "source": [
    "##### - Stereotype Category Distribution:\n",
    "    - All 16 stereotype categories are well represented in the dataset, ranging from 206 to 290 instances each.\n",
    "    - No major imbalances were found, which ensures fairness and robustness for downstream analysis.\n",
    "##### - Sentence Length Distribution:\n",
    "    - Most sentences are around 5-15 words long.\n",
    "    - A few outliers (very short or very long sentences) were identified and removed (~110 sentences) to prevent noise in embedding generation.\n",
    "##### - Word Frequency Analysis:\n",
    "    - Words like care, home, and felt were more common in Women stereotypes, while work, money, and team were more frequent in Men stereotypes.\n",
    "    - Per-category analysis further confirmed that the most frequent words aligned well with the stereotype labels, validating the mapping from StereoSet to GEST.\n",
    "##### - t-SNE Visualization – GPT-2 vs LLaMA-2 Embeddings:\n",
    "    - GPT-2 embeddings showed little structure; all stereotype categories and gender groups were highly intermixed.\n",
    "    - LLaMA-2 embeddings revealed more noticeable clusters. This clustering suggests that LLaMA-2 is already encoding stereotypical signals in its internal representation space — even before any generation or classification step.\n",
    "##### - Justification of research question:\n",
    "    - These EDA results provide early evidence that large language models like LLaMA-2 may internalize gender biases at the embedding level.\n",
    "    - The differences between GPT-2 and LLaMA-2 also highlight the potential impact of instruction tuning on how stereotypes are encoded.\n",
    "    - This strongly justifies a deeper investigation into how and where gender stereotypes are represented — comparing both prompt-based outputs and internal embeddings — as outlined in the rest of the study."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:neuralnetworks]",
   "language": "python",
   "name": "conda-env-neuralnetworks-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
